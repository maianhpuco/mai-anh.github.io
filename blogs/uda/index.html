<!DOCTYPE html>
<html lang="en" dir="auto">

<head>
        <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Domain adaptation (Part 1 - Covariate Shift) | Mai-Anh&#39;s blog about Machine Learning</title>
<meta name="keywords" content="">
<meta name="description" content="TL;DR: This blog focuses on the topic of Domain Adaptation, specifically in the unsupervised case and classification task. However, it does not cover semi-domain adaptation at this time. Domain Adaptation is a common challenge in machine learning that arises when there is a distribution mismatch between the training and testing data.
This blog post is inspired by the Stanford CS330 Deep Multi-Task &amp; Meta Learning lecture series, specifically Lecture 13 and 14.">
<meta name="author" content="">
<link rel="canonical" href="/blogs/uda/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css" integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe&#43;FVUFzPh7U=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="apple-touch-icon" href="apple-touch-icon.png">
<link rel="mask-icon" href="safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Domain adaptation (Part 1 - Covariate Shift)" />
<meta property="og:description" content="TL;DR: This blog focuses on the topic of Domain Adaptation, specifically in the unsupervised case and classification task. However, it does not cover semi-domain adaptation at this time. Domain Adaptation is a common challenge in machine learning that arises when there is a distribution mismatch between the training and testing data.
This blog post is inspired by the Stanford CS330 Deep Multi-Task &amp; Meta Learning lecture series, specifically Lecture 13 and 14." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/blogs/uda/" /><meta property="article:section" content="blogs" />
<meta property="article:published_time" content="2023-06-09T23:51:28+07:00" />
<meta property="article:modified_time" content="2023-06-09T23:51:28+07:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Domain adaptation (Part 1 - Covariate Shift)"/>
<meta name="twitter:description" content="TL;DR: This blog focuses on the topic of Domain Adaptation, specifically in the unsupervised case and classification task. However, it does not cover semi-domain adaptation at this time. Domain Adaptation is a common challenge in machine learning that arises when there is a distribution mismatch between the training and testing data.
This blog post is inspired by the Stanford CS330 Deep Multi-Task &amp; Meta Learning lecture series, specifically Lecture 13 and 14."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Blogs",
      "item": "/blogs/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Domain adaptation (Part 1 - Covariate Shift)",
      "item": "/blogs/uda/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Domain adaptation (Part 1 - Covariate Shift)",
  "name": "Domain adaptation (Part 1 - Covariate Shift)",
  "description": "TL;DR: This blog focuses on the topic of Domain Adaptation, specifically in the unsupervised case and classification task. However, it does not cover semi-domain adaptation at this time. Domain Adaptation is a common challenge in machine learning that arises when there is a distribution mismatch between the training and testing data.\nThis blog post is inspired by the Stanford CS330 Deep Multi-Task \u0026amp; Meta Learning lecture series, specifically Lecture 13 and 14.",
  "keywords": [
    
  ],
  "articleBody": " TL;DR: This blog focuses on the topic of Domain Adaptation, specifically in the unsupervised case and classification task. However, it does not cover semi-domain adaptation at this time. Domain Adaptation is a common challenge in machine learning that arises when there is a distribution mismatch between the training and testing data.\nThis blog post is inspired by the Stanford CS330 Deep Multi-Task \u0026 Meta Learning lecture series, specifically Lecture 13 and 14. I watch the lectures, then conducting codes and experiments for a more practical learning.\nThe blog series will be divided into three parts: **Covariate Shift, Adversarial Training and CycleGAN **.In each part, I will provide both theoretical explanations and practical code examples for understanding and implementing these concepts.\nThis is the first technical blog I have written for learning and sharing purposes, and mistakes may occur. I hope to receive any feedback. Let’s learn together!\n1. What is Domain Adaptation? Definition Domain adaptation a common issue when the characteristics or distributions of the training and testing datasets are different.\nIn domain adaptation, the source domain is where we have examples with labels that we can use to train our model (usually train set). The target domain is where we want our model to work well, even if we don’t have many or any labeled examples from that domain (usually validation set). The goal is to use what we learned from the source domain to make our model perform better in the target domain.\nDomain Adaptation is the task of adapting models across domains. This is motivated by the challenge where the test and training datasets fall from different data distributions due to some factor. Domain adaptation aims to build machine learning models that can be generalized into a target domain and dealing with the discrepancy across domain distributions. Source: paperwithcode.com\nExample 1: let’s consider a competition where you train a model on a dataset of colored images (MNIST-M). However, during evaluation, you are provided with black and white images (MNIST). Similarly, you may encounter a situation where you are given a grayscale image, but you need to build a model for color images.\nImage 1 - Training dataset: MNIST-M; MNIST-M is created by combining MNIST digits with the patches randomly extracted from color photos of BSDS500 as their background. It contains 59,001 training and 90,001 test images.:\nExample 2: imagine you develop a model to classify different types of white blood cells using data from one hospital. However, when you try to use the same model in another hospital, the imaging techniques and conditions may differ, resulting in a decrease in performance. This situation becomes more complex when dealing with three datasets: Raabin, BCCD, and LISC, which may originate from various countries, hospitals, and equipment.\nSome samples of the WBCs in Raabin-WBC, LISC, and BCCD datasets; Source\nThe problem setting of limited data has gained significant attention and importance in recent times. This blog post is inspired by the Stanford CS330 Deep Multi-Task \u0026 Meta Learning lecture series, specifically Lecture 13 and 14. I followed a similar structure to the lectures and mathematical explaination. Then I conducted an experiment using the MNIST-M and MNIST datasets.\nThis series of blog will have 3 part: This blog series will be divided into three parts:\nPart 1: Covariate Shift (including theoretical background and code) Part 2: Adversarial Training in Domain Adaptation (including theoretical background and code) Part 3: Cycle-GAN on Domain Translation (including theoretical background and, hopefully, code if I can complete it on time) Notation Let’s get used to some notation (read it slowly!): We have a data sample x and it label y. Model hypothesis: \\( f_{\\theta}(x) \\) . Where \\( f_{\\theta}\\) , can be any model, linear regression, SVM…or any Deep Learning model. Loss on 1 sample: \\( \\mathcal{L}(f_{\\theta}(x), y) \\) . Which measure distance between model output and it ground truth label. Distribution of \\(\\color{teal}{\\text{train set (or Source dataset)}}\\) : \\(\\color{teal}{P_{S}(x, y)} \\) Distribution of \\(\\color{magenta}{\\text{test set (or Target dataset)}}\\) : \\(\\color{magenta}{P_{T}(x, y)} \\) We then use expectation to calculate the loss function for the train and test sets:\nLoss (or error) function on \\(\\color{teal}{\\text{train set}}\\) : \\( \\epsilon_{\\color{teal}{S}}(f_{\\theta})= E_{\\color{teal}{P_{T}(x, y)}} [\\mathcal{L}(f_{\\theta}(x), y )] \\) This loss funciton average the errors or losses of our model’s predictions over all the training samples.\nLoss (or error) function on \\(\\color{magenta}{\\text{test set}}\\) \\( \\epsilon_{\\color{magenta}{S}}(f_{\\theta})= E_{\\color{magenta}{P_{T}(x, y)}} [\\mathcal{L}(f_{\\theta}(x), y )].\\) .Similarly, we calculate the loss on the test set to evaluate how well our model generalizes to unseen data.\nGeneral Assumption of Domain Adatation:** The source and target domain are only differ in domain of the function. Which i.e: \\(\\color{teal}{P_{S}(y|x)} \\) = \\(\\color{magenta}{P_{T}(y|x)} \\) There exist a single hypothesis with low error. Source for this assumption: Stanford CS330 Deep Multi-Task \u0026 Meta Learning - Domain Adaptation l 2022 I Lecture 13 , 2. Covariate Shift Problem Problem setting This refers to a domain adaptation problem setting where the distribution of training set and testing set are different. Usually in machine learning, it is assumed that the training and testing data have the same distribution. However, when there is a covariate shift, this assumption is violated, and it can lead to decreased performance of machine learning models.\nCovariate Shift occur when a model have distribution of train set differenet from its test set\nAssumption: The assumption of the covariate shift problem is that the support (range of values) of the source domain is larger than or equal to the support of the target domain. This means that the source domain covers a wider range of possible input features compared to the target domain.\nSolution Our Objective : Minimize \\( \\epsilon_{\\color{magenta}{T}}(f_{\\theta}) \\) under the assumption the distribution of train set and test set are not similar. however, there is another asssumption which will be prsent in the end of the section.\n💡 A simple solution is: re-weight the samples in the training set based on their likelihood of being representative of the test set (reweighting score), which mean assigning higher weights to samples that are more likely to be representative of the test. Doing so, we are assigning a higher prioritize for samples that are having more predicting ability on test set.\nMathematical ground For those who care about the “WHY”, let’s move to the this section. For those who are more interested in the “HOW”, you can skip ahead to the Implementation section.\nLet do some math together. Don’t worries, we can do it!\nFirstly, we recall the formular of expectation for a function:\nExpectation of a function: \\[E[g(Z)] = \\int f(z) \\cdot g(z) \\, dz \\] \\(g(z) \\) : function of Random Variable \\(X\\) \\(E[g(Z)] \\) : the expectation of the function \\(f(z) \\) : is the probability density function (PDF) of the random variable. The integral is taken over the entire range of possible values of \\( Z\\) Secondly, let go back to the objective and break down the mathematical expressions of the objectives:\n\\[\\epsilon_{\\color{magenta}{T}}(f_{\\theta}) = E_{\\color{magenta}{P_{T}(x, y)}}[\\mathcal{L}(f_{\\theta}(x), y )] =\\int \\color{magenta}{P_{T}(x, y)} \\mathcal{L}(f_{\\theta}(x), y ) dx dy \\\\ \\it{\\color{gray}{ \\text{#in this step, we expand the formular of expectation in the form of intergal}} }\\] \\[ =\\int \\color{magenta}{P_{T}(x, y)} \\frac{\\color{teal}{P_{S}(x, y)}} {\\color{teal}{P_{S}(x, y)}} \\mathcal{L}(f_{\\theta}(x), y) dx dy \\] \\[= \\int {\\color{teal}{P_{S}(x, y)}} \\frac{\\color{magenta}{P_{T}(x, y)}} {\\color{teal}{P_{S}(x, y)}} \\mathcal{L}(f_{\\theta}(x), y) dx dy \\\\ \\it{ \\color{gray}{ \\text{# in these 2 rows, we adding 1 which is also}} \\frac{\\color{teal}{P_{S}(x, y)}} {\\color{teal}{P_{S}(x, y)}} \\color{gray}{\\text{and then modify the position}} } \\] \\[ \\it{ \\color{gray}{ \\text{#to understand next lines, we need to revisit the expectation formular for a function E[g(x)] above: } \\\\ \\int \\color{brown}{f(z)} \\cdot \\color{pink}{g(z)}\\, dz = E_{Z}[g(z)] } , \\color{gray}{ \\text{similarly, let change the color code of the line above, we have } \\\\ \\int \\color{brown}{P_{S}(x, y)} \\cdot \\color{pink}{ \\frac{{P_{T}(x, y)}}{{P_{S}(x, y)}} \\mathcal{L}(f_{\\theta}(x), y) }\\, dx dy } \\\\ \\color{gray}{\\text{Here we have pdf: } \\color{brown}{f(z) = P_{S}(x, y)} \\text{ and a function of X and Y are : } \\color{pink}{g(x) = \\frac{{P_{T}(x, y)}}{{P_{S}(x, y)}} \\mathcal{L}(f_{\\theta}(x), y) }\\ } } \\] \\[ = E_{\\color{teal}{P_{S}(x, y)}} [ \\frac{\\color{magenta}{P_{T}(x, y)}} {\\color{teal}{P_{S}(x, y)}} \\mathcal{L}(f_{\\theta}(x), y) ] \\\\ \\] \\[ = E_{\\color{teal}{P_{S}(x, y)}} [ \\frac{\\color{magenta}{P_{T}(x | y) P_{T}(y)}} {\\color{teal}{P_{S}(x| y) P_{S}(y)}} \\mathcal{L}(f_{\\theta}(x), y) ] \\\\ \\it{\\color{gray}{ \\text{using Bayes Rules}}} \\] \\[ = E_{\\color{teal}{P_{S}(x, y)}} [ \\frac{\\color{magenta}{P_{T}(x)}} {\\color{teal}{P_{S}(x)}} \\mathcal{L}(f_{\\theta}(x), y)] \\\\ \\it{\\color{gray}{ \\text{we can reduce term above by assumption of Domain Adaption is } \\color{teal}{P_{S}(x | y)} = \\color{magenta}{P_{T}(x | y)} } } \\] How to compute: \\(\tE_{\\color{teal}{P_{S}(x, y)}} [ \\frac{\\color{magenta}{P_{T}(x)}} {\\color{teal}{P_{S}(x)}} \\mathcal{L}(f_{\\theta}(x), y)] \\) ?\nWe up-weight on samples on train set with high likelihood on target distribution (hight\n\\(\t\\color{teal}{P_{S}(x)} \\) ) and low likelihood on source distribution (low \\(\t\\color{magenta}{P_{T}(x)} \\) ). This is called Important Sampling (Importance Sampling is technique in Numerical Methods (a subject I almost failed in school 😅)\nThen, next question, how to Estimate this Proportion \\(\\frac{\\color{magenta}{P_{T}(x)}} {\\color{teal}{P_{S}(x)}} \\) To estimate this proportion, we can train a domain classifier to distinguish between the source and target domains. A domain classifier is a model that is trained to classify samples into their respective domains, typically the source and target domains. When the domain classifier outputs a prediction of 0, it means that the sample is classified as belonging to the source domain. A prediction of 1 indicates that the sample is classified as belonging to the target domain.\nAlgorithm\n\\[ - Step 1: \\text{Train a Domain Classifier} : cls(\\text{source} | x) \\\\ - Step 2: \\text{Reweight the Loss function with} \\frac{1-cls(\\text{source} | x)}{cls(\\text{source} | x)} \\\\ (Note: \\color{magenta}{p(\\text{target} | X_{i})} = 1 - \\color{teal}{p(\\text{source} | X_{i})}) \\] Proof of why we can perform Importance Sampling in the problem setting using a Domain Classifier:\nWe have \\[ \\color{magenta}{p_{T}(X) = P(X | \\text{domain = T}) = \\frac{{p(\\text{domain = T} | X) \\cdot p(X)}}{{p(\\text{domain = target})}}} \\] \\[ \\color{teal}{p_{S}(X) = P(X | \\text{domain = S}) = \\frac{{p(\\text{domain = S} | X) \\cdot p(X)}}{{p(\\text{domain = source})}}} \\] Then \\[ \\frac{{\\color{magenta}{p_{T}(x)}}}{{\\color{teal}{p_{S}(X)}}} = \\frac{{\\color{magenta}{p(X | \\text{domain = T})}}}{{\\color{teal}{p(X | \\text{domain = S})}}} \\] \\[= \\color{magenta}{\\frac{{p(\\text{domain = T} | X) \\cdot p(X)}}{{p(\\text{domain = target})}}} \\cdot \\color{teal}{\\frac{{p(\\text{domain = source})}}{{p(\\text{domain = S} | X) \\cdot p(X)}}} \\] \\[ = \\color{magenta}{\\frac{{p(\\text{domain = T} | X)}}{{p(\\text{domain = target})}}} \\cdot \\color{teal}{\\frac{{p(\\text{domain = source})}}{{p(\\text{domain = S} | X)}}} \\] \\[ = \\frac{{\\color{magenta}{p(\\text{domain = T} | X)}}}{{\\color{teal}{p(\\text{domain = S} | X)}}} \\cdot \\frac{{\\color{teal}{p(\\text{domain = source})}}}{{\\color{magenta}{p(\\text{domain = target})}}} \\] \\[ = \\frac{{\\color{magenta}{p(\\text{domain = T} | X)}}}{{\\color{teal}{p(\\text{domain = S} | X)}}} \\cdot \\text{constant} \\] Implementation: Model So, to simplify, we will just need:\nModel 1 - a domain classifer: This model classifies whether a given sample of data is coming from the source domain or the target domain. An output value of 1 indicates that the given sample is predicted to be from the source domain, otherwise it is predicted as target domain. We then use the probability output of the classifier as a reweighting score for each sample to “reweight” the loss function for the label classifier.\n\\[ \\text{Reweight the Loss function with} = \\frac{ \\color{teal}{1-{cls(\\text{source} | x)}} }{ \\color{teal}{ cls(\\text{source} | x)} } \\] \\( \\color{teal}{1-{cls(\\text{source| x)}}} \\) : represents the probability or confidence that a given sample “x” belongs to the target domain.\nModel 2 - a label classifier: This model performs classification and incorporates a reweight score by multiplying it in the second step mentioned above using ReweightLossfunction Experiment Details We will apply the solution on mnist dataset with 2 settings:\nSetting 1 MNIST-M -\u003e MNIST: MNIST-M as source domain, MNIST as target domain Setting 2 MNIST -\u003e MNIST-M: MNIST as source domain, MNIST-M as target domain (code bellow are for setting 1, please check for both setting) Model 1- Domain Classifier Let create a simple CNN for the Domain Classifer import torch import torch.nn as nn class DomainClassifier(nn.Module): def __init__(self): super(DomainClassifier, self).__init__() self.feature_extractor = nn.Sequential( nn.Conv2d(3, 64, kernel_size=5), nn.BatchNorm2d(64), nn.MaxPool2d(2), nn.ReLU(True), nn.Conv2d(64, 50, kernel_size=5), nn.BatchNorm2d(50), nn.Dropout2d(), nn.MaxPool2d(2), nn.ReLU(True) ) self.classifier = nn.Sequential( nn.Linear(50 * 4 * 4, 100), nn.BatchNorm1d(100), nn.ReLU(True), nn.Dropout2d(), nn.Linear(100, 100), nn.BatchNorm1d(100), nn.ReLU(True), nn.Linear(100, 2), nn.Softmax(dim=1) ) def forward(self, input_img): feature = self.feature_extractor(input_img) feature = feature.view(feature.size(0), -1) # Flatten the feature tensor domain_prob = self.classifier(feature) # domain_prob = torch.softmax(domain_output, dim=1) return domain_prob We create the TargetSourceDatset to train the Domain Classifier from torch.utils.data import Dataset, DataLoader class TargetSourceDataset(Dataset): def __init__(self, target_dataset, source_dataset): self.target_dataset = target_dataset self.source_dataset = source_dataset def __len__(self): return len(self.target_dataset) + len(self.source_dataset) def __getitem__(self, index): if index \u003c len(self.target_dataset): image, _ = self.target_dataset[index] domain_label = 0 # Target domain label else: source_index = index - len(self.target_dataset) image, _ = self.source_dataset[source_index] domain_label = 1 # Source domain label return image, domain_label # Create the combined dataset target_source_dataset = TargetSourceDataset(target_dataloader.dataset, source_dataloader.dataset) # Create the combined dataloader target_source_dataloader = DataLoader(target_source_dataset, batch_size=batch_size, shuffle=True) Training Domain Classifier: lr = 1e-4 batch_size = 128 image_size = 28 n_epoch = 2 domain_classifier = DomainClassifier().to(device) loss = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(domain_classifier.parameters(), lr=lr) LOSS_DOMAIN = [] ACC_DOMAIN = [] for epoch in range(n_epoch): correct = 0 total_samples = 0 len_target_source_dataloader = len(target_source_dataloader) for i, (img, label) in enumerate(target_source_dataloader): batch_size = len(img) input_img = img.to(device) label_output = domain_classifier(input_img) label_pred = torch.argmax(label_output, dim=1) label = label.to(device) correct += (label_pred == label).sum().item() total_samples += len(label) label_acc = 100.0 * correct / total_samples _loss = loss(label_output, label) ACC_DOMAIN.append(label_acc) LOSS_DOMAIN.append(_loss.item()) optimizer.zero_grad() _loss.backward() optimizer.step() if (i % 100) == 0 and i!= 0: print(f'epoch: {epoch+1}, [iter: {i:03d} / all {len_target_source_dataloader}], ' f'loss_label: {_loss.item():.4f}, ' f'| label acc: {label_acc:.4f}') We visualize output of the Domain Classifier, after computing the re-weighting score for each sample. As the images plotted, the images that have higher reweights score to the domain output are very similar to the TARGET domain. Top highest re-weighting score\nTop lowest re-weighting score\nLoss function We create the loss function as:\nclass ReweightedCrossEntropyLoss(nn.Module): def __init__(self): super(ReweightedCrossEntropyLoss, self).__init__() self.loss_function = nn.CrossEntropyLoss() def forward(self, input, target, domain_output): # Calculate the standard CrossEntropyLoss loss = self.loss_function(input, target) # Calculate the weight as domain_output[:, 0] / domain_output[:, 1] weight = domain_output[:, 0] / domain_output[:, 1] # Apply the weight factor weighted_loss = (weight * loss).mean() return weighted_loss However, the important weight can be blow up if the target domain is large which cause imbalance for model learning. There is several solutions mentions in lecture Covariate Shift (David S. Rosenberg (NYU: CDS))\nTo address this issue and mitigate the imbalance, we use both clipping and square root transformations in the loss function. Clipping can limit the range of the reweight scores, preventing extreme values from dominating the training process. Applying the square root transformation helps to reduce the disparity between large and small reweight scores, promoting a more balanced influence of different samples on the model’s learning.\nFinal loss function after modification:\nclass ReweightedCrossEntropyLoss(nn.Module): def __init__(self): super(ReweightedCrossEntropyLoss, self).__init__() self.loss_function = nn.CrossEntropyLoss(reduction='none') def forward(self, input, label, domain_output, threshold, max_value): # Calculate the standard CrossEntropyLoss loss = self.loss_function(input, label) source_prob = domain_output[:, 1] target_prob = domain_output[:, 0] weights = torch.sqrt(target_prob / (source_prob + 1e-8)) # Add a small epsilon to avoid division by zero weights = torch.where(weights \u003c threshold, torch.tensor(threshold), weights) weights = torch.where(weights \u003e max_value, torch.tensor(max_value), weights) weighted_loss = (weights * loss).mean() return weighted_loss Model 2: Label Classifier Label Classifier: a simple and shallow one class LabelClassifier(nn.Module): def __init__(self): super(LabelClassifier, self).__init__() self.feature_extractor = nn.Sequential( nn.Conv2d(3, 16, kernel_size=3), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Conv2d(16, 32, kernel_size=3), nn.ReLU(inplace=True), nn.MaxPool2d(2) ) self.classifier = nn.Sequential( nn.Linear(32 * 5 * 5, 100), nn.ReLU(inplace=True), nn.Linear(100, 10) ) def forward(self, input_img): feature = self.feature_extractor(input_img) feature = feature.view(feature.size(0), -1) # Flatten the feature tensor output = self.classifier(feature) return output Train the Label Classifier We create CombinedDataset: placedholder for input of the label classifier which would include source images, label of source image, domain label (1 for source and 0 for target) import torch import matplotlib.pyplot as plt from torch.utils.data import DataLoader, Dataset # Define a custom dataset for the combined data class CombinedDataset(Dataset): def __init__(self, images, labels, domain_outputs): self.images = images self.labels = labels self.domain_outputs = domain_outputs def __getitem__(self, index): image = self.images[index] label = self.labels[index] domain_output = self.domain_outputs[index] return image, label, domain_output def __len__(self): return len(self.images) Run the prediction of Domain Classifier on Source Domain and save result in the CombinedDataset\n# Calculate overall accuracy total_correct = 0 total_samples = 0 domain_classifier.eval() # Set the model to evaluation mode domain_outputs = [] source_images_list = [] source_labels_list = [] with torch.no_grad(): for images, source_labels in source_dataloader: input_images = images.to(device) labels = torch.ones(images.size(0), dtype=torch.long).to(device) domain_output = domain_classifier(input_images) _, domain_pred = torch.max(domain_output, 1) total_correct += (domain_pred == labels).sum().item() total_samples += len(labels) # print(100.0 * correct/total) domain_outputs.append(domain_output) source_images_list.append(input_images) source_labels_list.append(source_labels) # Concatenate the domain outputs, images, and labels combined_domain_output = torch.cat(domain_outputs, dim=0) combined_source_images = torch.cat(source_images_list, dim=0) combined_source_labels = torch.cat(source_labels_list, dim=0) combined_dataset = CombinedDataset(combined_source_images, combined_source_labels, combined_domain_output) accuracy = 100.0 * total_correct / total_samples print(f\"Accuracy of domain classifier: {accuracy:.4f}%\") batch_size = 128 combined_dataloader = DataLoader( combined_dataset, batch_size=batch_size, shuffle=True ) Train the Label Classifier, the prediction was run on TARGET domain and displayed the accuracy after each epoch; We perform 5 runs and record the average accuracy thes = 3 max_value = 4 lr = 1e-3 batch_size = 128 image_size = 28 n_epoch = 5 output_accuracies = [] for run_num in range(5) : LOSS_LABEL = [] ACC_LABEL = [] # Model, Loss, and Optimizer label_classifier = LabelClassifier().to(device) loss_label = ReweightedCrossEntropyLoss() optimizer = optim.Adam(label_classifier.parameters(), lr=lr) # Training loop input_dataloader = combined_dataloader acc_result = [] for epoch in range(n_epoch): correct = 0 total_samples = 0 data_iter = iter(input_dataloader) for i in range(len(input_dataloader)): len_dataloader = len(input_dataloader) data_source = next(data_iter) img, label, domain_clf_output = data_source batch_size = len(img) input_img = img.to(device) label = label.to(device) label_output = label_classifier(input_img) label_pred = torch.argmax(label_output, dim=1) correct += (label_pred == label).sum().item() total_samples += batch_size label_acc = 100.0 * correct / total_samples with torch.no_grad(): detached_domain_output = domain_clf_output.detach() loss = loss_label(label_output, label, domain_clf_output, thes, max_value) ACC_LABEL.append(label_acc) LOSS_LABEL.append(loss) optimizer.zero_grad() loss.backward() optimizer.step() if (i % 200) == 0 and i != 0: print(f'epoch: {epoch+1}, [iter: {i:03d} / all {len_dataloader}], ' f'loss label: {loss.item():.4f}, ' f'| label acc (SOURCE): {label_acc:.4f}') # run on test set after each epochs to monitor result on test set (target set) domain_outputs = [] source_train_images_list = [] source_train_labels_list = [] total_correct = 0 total_samples = 0 label_classifier.eval() for batch_data in target_dataloader: train_images, train_labels = batch_data batch_size = len(train_labels) input_images = train_images.to(device) class_label = train_labels.to(device) with torch.no_grad(): pred_output = label_classifier(input_images) class_pred = torch.argmax(pred_output, dim=1) correct = (class_pred == class_label).sum().item() total = len(class_label) total_correct += correct total_samples += total accuracy = 100.0 * total_correct / total_samples print(f\"Accuracy of label classifier (on TARGET) with reweight loss after epoch {epoch+1}: {accuracy:.4f}%\") acc_result.append(accuracy) output_accuracies.append(accuracy) print(\"Run num: \", run_num, ', '.join([f'{i:.4f}' for i in acc_result])) print(\"------------------------------------------\") print(\"\\n\") print(\"-\u003e Accuracies of 5 runs: \", ', '.join([f'{i:.2f}' for i in output_accuracies])) _acc = np.array(output_accuracies) _avg = np.mean(_acc, axis=0) print(f\"-\u003e Average accuracy of 5 runs: {_avg:.2f}\") \u003e\u003e ------------------------------------------ -\u003e Accuracies of 5 runs: 96.87, 97.09, 96.80, 97.31, 96.86 -\u003e Average accuracy of 5 runs: 96.99 Comparision Now, let compare with Traditional Implementation (without the Domain Classifer) by using the standard CrossEntropyLoss\nimport torch.optim as optim # Hyperparameters lr = 1e-3 batch_size = 128 image_size = 28 n_epoch = 5 output_accuracies = [] for run_num in range(5): output = [] # Model, Loss, and Optimizer traditional_label_classifier = LabelClassifier().to(device) loss_label_no_dc = nn.CrossEntropyLoss() optimizer = optim.Adam(traditional_label_classifier.parameters(), lr=lr) # Training loop CLASSICAL_LOSS_LABEL = [] CLASSICAL_ACC_LABEL = [] for epoch in range(n_epoch): correct = 0 total_samples = 0 input_dataloader = combined_dataloader data_iter = iter(input_dataloader) for i in range(len(input_dataloader)): len_dataloader = len(input_dataloader) data_source = next(data_iter) img, label, _= data_source batch_size = len(img) input_img = img.to(device) label = label.to(device) label_output = traditional_label_classifier(input_img) label_pred = torch.argmax(label_output, dim=1) correct += (label_pred == label).sum().item() total_samples += len(label) label_acc = 100.0 * correct / total_samples loss = loss_label_no_dc(label_output, label) CLASSICAL_ACC_LABEL.append(label_acc) CLASSICAL_LOSS_LABEL.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() if (i % 200) == 0 and i != 0: print(f'epoch: {epoch+1}, [iter: {i:03d} / all {len_dataloader}], ' f'loss label: {loss.item():.4f}, ' f'| label acc (SOURCE): {label_acc:.4f}') # Evaluation on target dataset after each epoch domain_outputs = [] source_train_images_list = [] source_train_labels_list = [] total_correct = 0 total_samples = 0 traditional_label_classifier.eval() for batch_data in target_dataloader: train_images, train_labels = batch_data batch_size = len(train_labels) input_images = train_images.to(device) class_label = train_labels.to(device) with torch.no_grad(): pred_output = traditional_label_classifier(input_images) class_pred = torch.argmax(pred_output, dim=1) correct = (class_pred == class_label).sum().item() total = len(class_label) total_correct += correct total_samples += total accuracy = 100.0 * total_correct / total_samples print(f\"- Accuracy of label classifier (predict on TARGET) after epoch {epoch+1}: {accuracy:.2f}%\") output.append(accuracy) output_accuracies.append(accuracy) print('Run number', run_num, '|', ', '.join([f'{i:.2f}' for i in output])) print(\"\\n\") print(\"\\n\") print(\"-\u003e Accuracies of 5 runs: \", ', '.join([f'{i:.2f}' for i in output_accuracies])) _acc = np.array(output_accuracies) _avg = np.mean(_acc, axis=0) print(f\"-\u003e Average accuracy of 5 runs: {_avg:.2f}\") Result:\n-\u003e Accuracies of 5 runs: 96.83, 96.09, 96.69, 96.94, 96.79 -\u003e Average accuracy of 5 runs: 96.67 Result on 2 setting MNIST-M -\u003e MNIST: MNIST-M as source and\nAccuracy MNIST-M -\u003e MNIST MNIST -\u003e mnist-M Traditional (average of 5 runs) 96.67% 55.31% Covariate - Shift (average of 5 runs) 96.99% 57.22% Discussion and future parts Analysis: Before we continue, let’s address a question: \\[ \\color{red}{\\text{Why does the result of Covariate Shift} \\\\ \\text{perform well in the case of } \\text{MNIST-M} \\rightarrow \\text{MNIST} \\\\ \\text{but poorly in the case of MNIST} \\rightarrow {MNIST-M?}} \\] Comment: Let’s revisit the assumption of the covariate shift problem setting that is that the support (range of values) of the source domain is larger than or equal to the support of the target domain. We observe that in the MNIST-M (source) -\u003e MNIST (target) case, the model still captures features related to the target set because there are many samples that are similar to the test set (colorized images can generalize to black and white). However, in the MNIST (source) -\u003e MNIST-M (target) case, the source dataset does not cover the domain of the domain set (black and white cannot generalize to colorized images).\nIn the next blogs, we will explore two other powerful techniques to handle situations where the support of the test set is not within the training set, Domain Adversarial Training and CycleGAN.\nAccording to the table below, we can see that Domain Adversarial Training handles the situation very well in the MNIST -\u003e MNIST-M case. In the covariate shift problem, we focus on mapping the sample space, but with domain adversarial training, we concentrate on the feature space. We aim to find a feature space that is general enough to represent both domains (invariant domain features).\nAccuracy MNIST-M -\u003e MNIST MNIST -\u003e mnist-M Traditional (average of 5 runs) 96.67% 55.31% Covariate - Shift (average of 5 runs) 96.99% 57.22% Domain Adversarial Training 96.61% 72.08% In Part 3, we will delve into a more advanced technique related to GANs, known as CycleGAN, where we generate samples from domain A while incorporating domain information from another domain, B.\nIsn’t it interesting? Please Let me know your comment and feedback. See you in the next posts!\n",
  "wordCount" : "3768",
  "inLanguage": "en",
  "datePublished": "2023-06-09T23:51:28+07:00",
  "dateModified": "2023-06-09T23:51:28+07:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/blogs/uda/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Mai-Anh's blog about Machine Learning",
    "logo": {
      "@type": "ImageObject",
      "url": "favicon.ico"
    }
  }
}
</script> 
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="" accesskey="h" title="Mai-Anh&#39;s blog about Machine Learning (Alt + H)">Mai-Anh&#39;s blog about Machine Learning</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Domain adaptation (Part 1 - Covariate Shift)
    </h1>
    <div class="post-meta"><span title='2023-06-09 23:51:28 +0700 +07'>June 9, 2023</span>

</div>
  </header> 
  <div class="post-content"><!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h1 id="tldr">TL;DR:<a hidden class="anchor" aria-hidden="true" href="#tldr">#</a></h1>
<p>This blog focuses on the topic of <strong>Domain Adaptation</strong>, specifically in the <strong>unsupervised</strong> case and classification task. However, it does not cover semi-domain adaptation at this time. Domain Adaptation is a common challenge in machine learning that arises when there is a distribution mismatch between the training and testing data.</p>
<p>This blog post is inspired by the <a href="https://www.youtube.com/playlist?list=PLoROMvodv4rNjRoawgt72BBNwL2V7doGI">Stanford CS330 Deep Multi-Task &amp; Meta Learning lecture series</a>, specifically Lecture 13 and 14. I watch the lectures, then conducting codes and experiments for a more practical learning.</p>
<p>The blog series will be divided into three parts: **Covariate Shift, Adversarial Training and CycleGAN **.In each part, I will provide both theoretical explanations and practical code examples for understanding and implementing these concepts.</p>
<blockquote>
<p><em>This is the first technical blog I have written for learning and sharing purposes, and mistakes may occur. I hope to receive any feedback. Let&rsquo;s learn together!</em></p>
</blockquote>
<h1 id="1-what-is-domain-adaptation">1. What is Domain Adaptation?<a hidden class="anchor" aria-hidden="true" href="#1-what-is-domain-adaptation">#</a></h1>
<h2 id="definition">Definition<a hidden class="anchor" aria-hidden="true" href="#definition">#</a></h2>
<p>Domain adaptation a common issue when the <strong>characteristics or distributions of the training and testing datasets are different</strong>.</p>
<p>In domain adaptation, the <strong>source domain</strong> is where we have examples <strong>with labels that we can use to train our model</strong> (usually train set). The <strong>target domain</strong> is where we want our model to work well, even if we <strong>don&rsquo;t have many or any labeled examples from that domain</strong> (usually validation set). The goal is to use what we learned from the source domain to make our model perform better in the target domain.</p>
<blockquote>
<p><em>Domain Adaptation is the task of adapting models across domains. This is motivated by the challenge <strong>where the test and training datasets fall from different data distributions</strong> due to some factor. Domain adaptation aims to build machine learning models that can be generalized into a target domain and dealing with the discrepancy across domain distributions.</em>
<strong>Source: paperwithcode.com</strong></p>
</blockquote>
<p><strong>Example 1</strong>: let&rsquo;s consider a competition where you train a model on a dataset of colored images (MNIST-M). However, during evaluation, you are provided with black and white images (MNIST). Similarly, you may encounter a situation where you are given a grayscale image, but you need to build a model for color images.</p>
<figure class="align-center centered-caption">
    <img loading="lazy" src="/blogs/uda/intro.png#center"
         alt="Image Alt Text" width="600px"/> <figcaption>
            <p>Image 1 - Training dataset: <a href="https://www.kaggle.com/datasets/aquibiqbal/mnistm">MNIST-M</a>; MNIST-M is created by combining MNIST digits with the patches randomly extracted from color photos of BSDS500 as their background. It contains 59,001 training and 90,001 test images.:</p>
        </figcaption>
</figure>

<p><strong>Example 2</strong>: imagine you develop a model to classify different types of white blood cells using data from one hospital. However, when you try to use the same model in another hospital, the imaging techniques and conditions may differ, resulting in a decrease in performance. This situation becomes more complex when dealing with three datasets: Raabin, BCCD, and LISC, which may originate from various countries, hospitals, and equipment.</p>
<figure class="align-center centered-caption">
    <img loading="lazy" src="/blogs/uda/wbc-example.png#center"
         alt="Some samples of the WBCs in Raabin-WBC, LISC, and BCCD datasets; Source" width="400px"/> <figcaption>
            <p>Some samples of the WBCs in Raabin-WBC, LISC, and BCCD datasets; <a href="https://www.researchgate.net/publication/354960074_New_segmentation_and_feature_extraction_algorithm_for_classification_of_white_blood_cells_in_peripheral_smear_images">Source</a></p>
        </figcaption>
</figure>

<p>The problem setting of limited data has gained significant attention and importance in recent times. This blog post is inspired by the <a href="https://www.youtube.com/playlist?list=PLoROMvodv4rNjRoawgt72BBNwL2V7doGI">Stanford CS330 Deep Multi-Task &amp; Meta Learning lecture series</a>, specifically Lecture 13 and 14. I followed a similar structure to the lectures and mathematical explaination. Then I conducted an experiment using the MNIST-M and MNIST datasets.</p>
<p>This series of blog will have 3 part:
This blog series will be divided into three parts:</p>
<ul>
<li>Part 1: Covariate Shift (including theoretical background and code)</li>
<li>Part 2: Adversarial Training in Domain Adaptation (including theoretical background and code)</li>
<li>Part 3: Cycle-GAN on Domain Translation (including theoretical background and, hopefully, code if I can complete it on time)</li>
</ul>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="notation">Notation<a hidden class="anchor" aria-hidden="true" href="#notation">#</a></h2>
<p>Let&rsquo;s get used to some notation (read it slowly!):
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex">
</span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
</p>
<ul>
<li>We have a data sample x and it label y.</li>
<li>Model hypothesis: <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \( f_{\theta}(x) \) </span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
. Where  <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \( f_{\theta}\) </span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
, can be any model, linear regression, SVM&hellip;or any Deep Learning model.</li>
<li>Loss on 1 sample: <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \( \mathcal{L}(f_{\theta}(x), y) \) </span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
. Which measure distance between model output and it ground truth label.</li>
<li>Distribution of <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \(\color{teal}{\text{train set (or Source dataset)}}\) </span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
:
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \(\color{teal}{P_{S}(x, y)} \)</span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
</li>
<li>Distribution of <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \(\color{magenta}{\text{test set (or Target dataset)}}\) </span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
:<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \(\color{magenta}{P_{T}(x, y)} \) </span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
</li>
</ul>
<p>We then use expectation to calculate the loss function for the train and test sets:</p>
<ul>
<li>
<p>Loss (or error) function on  <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \(\color{teal}{\text{train set}}\) </span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
:
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex">
\(
\epsilon_{\color{teal}{S}}(f_{\theta})= E_{\color{teal}{P_{T}(x, y)}}
[\mathcal{L}(f_{\theta}(x), y )]
\)
</span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 

This loss funciton average the errors or losses of our model&rsquo;s predictions over all the training samples.</p>
</li>
<li>
<p>Loss (or error) function on <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \(\color{magenta}{\text{test set}}\) </span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex">
\(
\epsilon_{\color{magenta}{S}}(f_{\theta})= E_{\color{magenta}{P_{T}(x, y)}}
[\mathcal{L}(f_{\theta}(x), y )].\)
</span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
.Similarly, we calculate the loss on the test set to evaluate how well our model generalizes to unseen data.</p>
</li>
</ul>
<h2 id="general-assumption-of-domain-adatation">General Assumption of Domain Adatation:**<a hidden class="anchor" aria-hidden="true" href="#general-assumption-of-domain-adatation">#</a></h2>
<ul>
<li>The <strong>source</strong> and <strong>target</strong> domain are <strong>only differ in domain</strong> of the function. Which i.e:    <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \(\color{teal}{P_{S}(y|x)} \)</span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 

=   <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \(\color{magenta}{P_{T}(y|x)} \)</span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
</li>
<li>There exist a single hypothesis with low error.
Source for this assumption:  <a href="https://www.youtube.com/watch?v=Uk6MU_PLDMs&amp;list=PLoROMvodv4rNjRoawgt72BBNwL2V7doGI&amp;index=13&amp;t=272s">Stanford CS330 Deep Multi-Task &amp; Meta Learning - Domain Adaptation l 2022 I Lecture 13
</a>,</li>
</ul>
<h1 id="2-covariate-shift-problem">2. Covariate Shift Problem<a hidden class="anchor" aria-hidden="true" href="#2-covariate-shift-problem">#</a></h1>
<h2 id="problem-setting">Problem setting<a hidden class="anchor" aria-hidden="true" href="#problem-setting">#</a></h2>
<p>This refers to a domain adaptation problem setting where the <strong>distribution of training set and testing set are different</strong>. Usually in machine learning, it is assumed that the training and testing data have the same distribution. However, when there is a covariate shift, this assumption is violated, and it can lead to decreased performance of machine learning models.</p>
<!-- raw HTML omitted -->
<figure class="centered-caption">
    <img loading="lazy" src="/blogs/uda/intro-img.png"
         alt="Image Alt Text"/> <figcaption>
            <p>Covariate Shift occur when a model have distribution of train set differenet from its test set</p>
        </figcaption>
</figure>

<p><strong>Assumption</strong>: The assumption of the covariate shift problem is that <strong>the support (range of values) of the source domain is larger than or equal to the support of the target domain</strong>. This means that the source domain covers a wider range of possible input features compared to the target domain.</p>
<h2 id="solution">Solution<a hidden class="anchor" aria-hidden="true" href="#solution">#</a></h2>
<p><strong>Our Objective</strong> : Minimize <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex">
\(
 \epsilon_{\color{magenta}{T}}(f_{\theta}) \)  </span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
 under the assumption the distribution of train set and test set are not similar. however, there is another asssumption which will be prsent in the end of the section.</p>
<p>💡 A simple solution is: <strong>re-weight the samples in the training set based on their likelihood of being representative of the test set</strong> (<strong>reweighting score</strong>), which mean assigning higher weights to samples that are more likely to be representative of the test. Doing so,  we are assigning a higher prioritize for samples that are having more predicting ability on test set.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="mathematical-ground">Mathematical ground<a hidden class="anchor" aria-hidden="true" href="#mathematical-ground">#</a></h2>
<p><em>For those who care about the <strong>&ldquo;WHY&rdquo;</strong>, let&rsquo;s move to the this section. For those who are more interested in the <strong>&ldquo;HOW&rdquo;</strong>, you can <strong>skip ahead to the Implementation</strong> section.</em></p>
<p>Let do some math together. Don&rsquo;t worries, we can do it!</p>
<p><strong>Firstly, we recall the formular of expectation for a function:</strong></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<ul>
<li><strong>Expectation of a function</strong>:
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex">
 \[E[g(Z)] = \int f(z) \cdot g(z) \, dz \]
	</span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 

<ul>
<li><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \(g(z)   \) </span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
: function of Random Variable 	<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \(X\) </span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
</li>
<li><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \(E[g(Z)] \)</span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
: the expectation of the function</li>
<li><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \(f(z) \)</span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
: is the probability density function (PDF) of the random variable. The integral is taken over the entire range of possible values of 	<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \( Z\) </span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
</li>
</ul>
</li>
</ul>
<p><strong>Secondly, let go back to the objective and break down the mathematical expressions of the objectives:</strong></p>
<blockquote>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex">
		
	\[\epsilon_{\color{magenta}{T}}(f_{\theta}) = E_{\color{magenta}{P_{T}(x, y)}}[\mathcal{L}(f_{\theta}(x), y )] 
       =\int \color{magenta}{P_{T}(x, y)} \mathcal{L}(f_{\theta}(x), y ) dx dy \\  
					
				\it{\color{gray}{
												\text{#in this step, we expand the formular of expectation in the form of intergal}} 
						}\]
       \[ 
				=\int
						\color{magenta}{P_{T}(x, y)}
						\frac{\color{teal}{P_{S}(x, y)}}
									{\color{teal}{P_{S}(x, y)}} 
						\mathcal{L}(f_{\theta}(x), y) dx dy 
			 \] 

       \[= 
			 \int 
					{\color{teal}{P_{S}(x, y)}} 
					\frac{\color{magenta}{P_{T}(x, y)}}
							 {\color{teal}{P_{S}(x, y)}}
					\mathcal{L}(f_{\theta}(x), y) dx dy \\ 

				\it{
						\color{gray}{
							\text{# in these 2 rows, we adding 1 which is also}} 
							\frac{\color{teal}{P_{S}(x, y)}}
									{\color{teal}{P_{S}(x, y)}}
									\color{gray}{\text{and then modify the position}}
						}
				\]


			\[
				\it{
					\color{gray}{
							\text{#to understand next lines, we need to revisit the expectation formular for a function E[g(x)] above: } \\  
							\int 
									\color{brown}{f(z)} \cdot \color{pink}{g(z)}\, dz = E_{Z}[g(z)] }  , 
									\color{gray}{
															\text{similarly, let change the color code of the line above, we have  } \\ 
															\int \color{brown}{P_{S}(x, y)} \cdot 
																	 \color{pink}{
																					\frac{{P_{T}(x, y)}}{{P_{S}(x, y)}} \mathcal{L}(f_{\theta}(x), y) }\, dx dy
															} \\ 
									\color{gray}{\text{Here we have pdf: } 
															\color{brown}{f(z) = P_{S}(x, y)} 
															\text{ and a function of X and Y are : }
															\color{pink}{g(x) = 
																					\frac{{P_{T}(x, y)}}{{P_{S}(x, y)}} \mathcal{L}(f_{\theta}(x), y) }\
															} 
				}

			\]

       
      \[
				= E_{\color{teal}{P_{S}(x, y)}}  [ \frac{\color{magenta}{P_{T}(x, y)}}
																								{\color{teal}{P_{S}(x, y)}} 
																								\mathcal{L}(f_{\theta}(x), y) ] \\ 
			\]

       \[
					= E_{\color{teal}{P_{S}(x, y)}}  [ \frac{\color{magenta}{P_{T}(x | y) P_{T}(y)}}
																								{\color{teal}{P_{S}(x| y) P_{S}(y)}} 
																								\mathcal{L}(f_{\theta}(x), y) ] \\  

																				 
				\it{\color{gray}{
												\text{using Bayes Rules}}}
				\]


       \[
			 = E_{\color{teal}{P_{S}(x, y)}}  [ \frac{\color{magenta}{P_{T}(x)}}
																								{\color{teal}{P_{S}(x)}} 
																								\mathcal{L}(f_{\theta}(x), y)] \\


				\it{\color{gray}{
												\text{we can reduce term above by assumption of Domain Adaption is }
												\color{teal}{P_{S}(x | y)}  = \color{magenta}{P_{T}(x | y)}
												}
					}
												
				\] 
 </span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 

</blockquote>
<p><strong>How to compute</strong>: <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \(	E_{\color{teal}{P_{S}(x, y)}}  [ \frac{\color{magenta}{P_{T}(x)}}
																								{\color{teal}{P_{S}(x)}} 
																								\mathcal{L}(f_{\theta}(x), y)] \) </span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
 ?</p>
<p>We up-weight on samples on train set with <strong>high likelihood</strong>  on  target distribution (hight<br>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex">\(	 \color{teal}{P_{S}(x)} \) </span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
 ) and <strong>low likelihood</strong> on source distribution (low<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \(	\color{magenta}{P_{T}(x)} \) </span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
).  This is called <strong>Important Sampling</strong>  (Importance Sampling is technique in Numerical Methods (a subject I almost failed in school 😅)</p>
<p>Then, next question, how to Estimate this Proportion <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \(\frac{\color{magenta}{P_{T}(x)}}
				 {\color{teal}{P_{S}(x)}} 
	\) </span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
</p>
<p>To estimate this proportion, we can train a domain classifier to distinguish between the source and target domains. A domain classifier is a model that is trained to classify samples into their respective domains, typically the source and target domains. When the domain classifier outputs a prediction of 0, it means that the sample is classified as belonging to the source domain. A prediction of 1 indicates that the sample is classified as belonging to the target domain.</p>
<p><strong>Algorithm</strong></p>
<blockquote>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex"> \[ 
- Step 1: \text{Train a Domain Classifier} : cls(\text{source} | x) \\  
- Step 2: \text{Reweight the Loss function with}  \frac{1-cls(\text{source} | x)}{cls(\text{source} | x)} \\
(Note:  \color{magenta}{p(\text{target} | X_{i})} = 1 - \color{teal}{p(\text{source} | X_{i})})
	\]
</span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 

</blockquote>
<p><strong>Proof</strong>  of why we can perform Importance Sampling in the problem setting using a Domain Classifier:</p>
<blockquote>
<ul>
<li><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex">
We have
 \[
\color{magenta}{p_{T}(X) = P(X | \text{domain = T}) = \frac{{p(\text{domain = T} | X) \cdot p(X)}}{{p(\text{domain = target})}}}
\]

\[
\color{teal}{p_{S}(X) = P(X | \text{domain = S}) = \frac{{p(\text{domain = S} | X) \cdot p(X)}}{{p(\text{domain = source})}}}
\] 

 
</span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Then</li>
</ul>
</blockquote>
<blockquote>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex">

\[
\frac{{\color{magenta}{p_{T}(x)}}}{{\color{teal}{p_{S}(X)}}} = \frac{{\color{magenta}{p(X | \text{domain = T})}}}{{\color{teal}{p(X | \text{domain = S})}}} 
\] 
\[= \color{magenta}{\frac{{p(\text{domain = T} | X) \cdot p(X)}}{{p(\text{domain = target})}}} \cdot \color{teal}{\frac{{p(\text{domain = source})}}{{p(\text{domain = S} | X) \cdot p(X)}}}
\]

\[
= \color{magenta}{\frac{{p(\text{domain = T} | X)}}{{p(\text{domain = target})}}} \cdot \color{teal}{\frac{{p(\text{domain = source})}}{{p(\text{domain = S} | X)}}}
\]

\[
= \frac{{\color{magenta}{p(\text{domain = T} | X)}}}{{\color{teal}{p(\text{domain = S} | X)}}} \cdot \frac{{\color{teal}{p(\text{domain = source})}}}{{\color{magenta}{p(\text{domain = target})}}}
\]

\[
= \frac{{\color{magenta}{p(\text{domain = T} | X)}}}{{\color{teal}{p(\text{domain = S} | X)}}} \cdot \text{constant}
\]

</span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 

</blockquote>
<h1 id="implementation">Implementation:<a hidden class="anchor" aria-hidden="true" href="#implementation">#</a></h1>
<h2 id="model">Model<a hidden class="anchor" aria-hidden="true" href="#model">#</a></h2>
<p>So, to simplify, we will just need:</p>
<ul>
<li><strong>Model 1 - a domain classifer</strong>: This model classifies whether a given sample of data is coming from the source domain or the target domain. An output value of 1 indicates that the given sample is predicted to be from the source domain, otherwise it is predicted as target domain.</li>
</ul>
<p>We then use the probability output of the classifier as a reweighting score for each sample to &ldquo;reweight&rdquo; the loss function for the label classifier.</p>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex">
 \[
    \text{Reweight the Loss function with}  = \frac{
                                                    \color{teal}{1-{cls(\text{source} | x)}}
                                                    }{
                                                    \color{teal}{ cls(\text{source} | x)}
                                                    } 



\]
</span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 

<p><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex">  \( \color{teal}{1-{cls(\text{source| x)}}}  \)
</span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
:  represents the probability or confidence that a given sample &ldquo;x&rdquo; belongs to the target domain.</p>
<ul>
<li><strong>Model 2 - a label classifier</strong>: This model performs classification and incorporates a reweight score by multiplying it in the second step mentioned above using ReweightLossfunction</li>
</ul>
<h2 id="experiment-details">Experiment Details<a hidden class="anchor" aria-hidden="true" href="#experiment-details">#</a></h2>
<p>We will apply the solution on mnist dataset with 2 settings:</p>
<ul>
<li>Setting 1 <strong>MNIST-M -&gt; MNIST</strong>: MNIST-M as source domain, MNIST  as target domain</li>
<li>Setting 2 <strong>MNIST -&gt; MNIST-M</strong>: MNIST  as source domain, MNIST-M as target domain
(code bellow are for setting 1, please check <a href=""></a> for both setting)</li>
</ul>
<h2 id="model-1---domain-classifier">Model 1-  Domain Classifier<a hidden class="anchor" aria-hidden="true" href="#model-1---domain-classifier">#</a></h2>
<ul>
<li>Let create a simple CNN for the Domain Classifer</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DomainClassifier</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super(DomainClassifier, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>feature_extractor <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">64</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm2d(<span style="color:#ae81ff">64</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>MaxPool2d(<span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(<span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">50</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm2d(<span style="color:#ae81ff">50</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Dropout2d(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>MaxPool2d(<span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(<span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>classifier <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">50</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">100</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">100</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(<span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Dropout2d(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">100</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">100</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(<span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Softmax(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input_img):
</span></span><span style="display:flex;"><span>        feature <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>feature_extractor(input_img)
</span></span><span style="display:flex;"><span>        feature <span style="color:#f92672">=</span> feature<span style="color:#f92672">.</span>view(feature<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># Flatten the feature tensor</span>
</span></span><span style="display:flex;"><span>        domain_prob <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>classifier(feature)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># domain_prob = torch.softmax(domain_output, dim=1)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> domain_prob
</span></span></code></pre></div><ul>
<li>We create the <code>TargetSourceDatset</code> to train the Domain Classifier</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> Dataset, DataLoader
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">TargetSourceDataset</span>(Dataset):
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, target_dataset, source_dataset):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>target_dataset <span style="color:#f92672">=</span> target_dataset
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>source_dataset <span style="color:#f92672">=</span> source_dataset
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __len__(self):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> len(self<span style="color:#f92672">.</span>target_dataset) <span style="color:#f92672">+</span> len(self<span style="color:#f92672">.</span>source_dataset)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __getitem__(self, index):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> index <span style="color:#f92672">&lt;</span> len(self<span style="color:#f92672">.</span>target_dataset):
</span></span><span style="display:flex;"><span>            image, _ <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>target_dataset[index]
</span></span><span style="display:flex;"><span>            domain_label <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>  <span style="color:#75715e"># Target domain label</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            source_index <span style="color:#f92672">=</span> index <span style="color:#f92672">-</span> len(self<span style="color:#f92672">.</span>target_dataset)
</span></span><span style="display:flex;"><span>            image, _ <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>source_dataset[source_index]
</span></span><span style="display:flex;"><span>            domain_label <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>  <span style="color:#75715e"># Source domain label</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> image, domain_label
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create the combined dataset</span>
</span></span><span style="display:flex;"><span>target_source_dataset <span style="color:#f92672">=</span> TargetSourceDataset(target_dataloader<span style="color:#f92672">.</span>dataset, source_dataloader<span style="color:#f92672">.</span>dataset)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create the combined dataloader  </span>
</span></span><span style="display:flex;"><span>target_source_dataloader <span style="color:#f92672">=</span> DataLoader(target_source_dataset, batch_size<span style="color:#f92672">=</span>batch_size, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><ul>
<li>Training Domain Classifier:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-4</span>
</span></span><span style="display:flex;"><span>batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>
</span></span><span style="display:flex;"><span>image_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">28</span>
</span></span><span style="display:flex;"><span>n_epoch <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>domain_classifier <span style="color:#f92672">=</span> DomainClassifier()<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>loss <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>CrossEntropyLoss()
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>Adam(domain_classifier<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span>lr)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>LOSS_DOMAIN <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>ACC_DOMAIN <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(n_epoch):
</span></span><span style="display:flex;"><span>    correct <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    total_samples <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    len_target_source_dataloader <span style="color:#f92672">=</span> len(target_source_dataloader)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i, (img, label) <span style="color:#f92672">in</span> enumerate(target_source_dataloader):
</span></span><span style="display:flex;"><span>        batch_size <span style="color:#f92672">=</span> len(img)
</span></span><span style="display:flex;"><span>        input_img <span style="color:#f92672">=</span> img<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        label_output <span style="color:#f92672">=</span> domain_classifier(input_img)
</span></span><span style="display:flex;"><span>        label_pred <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(label_output, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        label <span style="color:#f92672">=</span> label<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        correct <span style="color:#f92672">+=</span> (label_pred <span style="color:#f92672">==</span> label)<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>        total_samples <span style="color:#f92672">+=</span> len(label)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        label_acc <span style="color:#f92672">=</span> <span style="color:#ae81ff">100.0</span> <span style="color:#f92672">*</span> correct <span style="color:#f92672">/</span> total_samples
</span></span><span style="display:flex;"><span>        _loss <span style="color:#f92672">=</span> loss(label_output, label)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        ACC_DOMAIN<span style="color:#f92672">.</span>append(label_acc)
</span></span><span style="display:flex;"><span>        LOSS_DOMAIN<span style="color:#f92672">.</span>append(_loss<span style="color:#f92672">.</span>item())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>        _loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (i <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span>) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">and</span> i<span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;epoch: </span><span style="color:#e6db74">{</span>epoch<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">, [iter: </span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">:</span><span style="color:#e6db74">03d</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> / all </span><span style="color:#e6db74">{</span>len_target_source_dataloader<span style="color:#e6db74">}</span><span style="color:#e6db74">], &#39;</span>
</span></span><span style="display:flex;"><span>                    <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;loss_label: </span><span style="color:#e6db74">{</span>_loss<span style="color:#f92672">.</span>item()<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">, &#39;</span>
</span></span><span style="display:flex;"><span>                    <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;| label acc: </span><span style="color:#e6db74">{</span>label_acc<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span></code></pre></div><p>We visualize output of the Domain Classifier, after computing the <strong>re-weighting score</strong> for each sample. <strong>As the images plotted, the images that have higher reweights score to the domain output are very similar to the TARGET domain.</strong>
<figure class="centered-caption">
    <img loading="lazy" src="/blogs/uda/mnist-m-highest-w.png"
         alt="Image Alt Text"/> <figcaption>
            <p>Top highest re-weighting score</p>
        </figcaption>
</figure>
</p>
<figure class="centered-caption">
    <img loading="lazy" src="/blogs/uda/mnist-m-lowest-w.png"
         alt="Image Alt Text"/> <figcaption>
            <p>Top lowest re-weighting score</p>
        </figcaption>
</figure>

<h2 id="loss-function">Loss function<a hidden class="anchor" aria-hidden="true" href="#loss-function">#</a></h2>
<p>We create the loss function as:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ReweightedCrossEntropyLoss</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super(ReweightedCrossEntropyLoss, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_function <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>CrossEntropyLoss()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input, target, domain_output):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Calculate the standard CrossEntropyLoss</span>
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>loss_function(input, target)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Calculate the weight as domain_output[:, 0] / domain_output[:, 1]</span>
</span></span><span style="display:flex;"><span>        weight <span style="color:#f92672">=</span> domain_output[:, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">/</span> domain_output[:, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Apply the weight factor</span>
</span></span><span style="display:flex;"><span>        weighted_loss <span style="color:#f92672">=</span> (weight <span style="color:#f92672">*</span> loss)<span style="color:#f92672">.</span>mean()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> weighted_loss
</span></span></code></pre></div><p>However, the important weight can be blow up if the target domain is large which cause imbalance for model learning. There is several solutions mentions in lecture  <a href="https://davidrosenberg.github.io/ttml2021/missing-data/5.covariate_shift.pdf">Covariate Shift (David S. Rosenberg (NYU: CDS))</a></p>
<p>To address this issue and mitigate the imbalance, we use both <strong>clipping</strong> and <strong>square root transformations</strong> in the loss function. Clipping can limit the range of the reweight scores, preventing extreme values from dominating the training process. Applying the square root transformation helps to reduce the disparity between large and small reweight scores, promoting a more balanced influence of different samples on the model&rsquo;s learning.</p>
<p>Final loss function after modification:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ReweightedCrossEntropyLoss</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super(ReweightedCrossEntropyLoss, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_function <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>CrossEntropyLoss(reduction<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input, label, domain_output, threshold, max_value):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Calculate the standard CrossEntropyLoss</span>
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>loss_function(input, label)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        source_prob <span style="color:#f92672">=</span> domain_output[:, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        target_prob <span style="color:#f92672">=</span> domain_output[:, <span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>        weights <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sqrt(target_prob <span style="color:#f92672">/</span> (source_prob <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-8</span>))  <span style="color:#75715e"># Add a small epsilon to avoid division by zero</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        weights <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>where(weights <span style="color:#f92672">&lt;</span> threshold, torch<span style="color:#f92672">.</span>tensor(threshold), weights)
</span></span><span style="display:flex;"><span>        weights <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>where(weights <span style="color:#f92672">&gt;</span> max_value, torch<span style="color:#f92672">.</span>tensor(max_value), weights)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        weighted_loss <span style="color:#f92672">=</span> (weights <span style="color:#f92672">*</span> loss)<span style="color:#f92672">.</span>mean()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> weighted_loss
</span></span></code></pre></div><h2 id="model-2-label-classifier">Model 2: Label Classifier<a hidden class="anchor" aria-hidden="true" href="#model-2-label-classifier">#</a></h2>
<ul>
<li>Label Classifier: a simple and shallow one</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LabelClassifier</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super(LabelClassifier, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>feature_extractor <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">16</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>MaxPool2d(<span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">32</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>MaxPool2d(<span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>classifier <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">32</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">5</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">100</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input_img):
</span></span><span style="display:flex;"><span>        feature <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>feature_extractor(input_img)
</span></span><span style="display:flex;"><span>        feature <span style="color:#f92672">=</span> feature<span style="color:#f92672">.</span>view(feature<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># Flatten the feature tensor</span>
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>classifier(feature)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> output
</span></span></code></pre></div><h2 id="train-the-label-classifier">Train the Label Classifier<a hidden class="anchor" aria-hidden="true" href="#train-the-label-classifier">#</a></h2>
<ul>
<li>We create <code>CombinedDataset</code>: placedholder for input of the label classifier which would include source images, label of source image, domain label (1 for source and 0 for target)</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> DataLoader, Dataset
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define a custom dataset for the combined data</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CombinedDataset</span>(Dataset):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, images, labels, domain_outputs):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>images <span style="color:#f92672">=</span> images
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>labels <span style="color:#f92672">=</span> labels
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>domain_outputs <span style="color:#f92672">=</span> domain_outputs
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __getitem__(self, index):
</span></span><span style="display:flex;"><span>        image <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>images[index]
</span></span><span style="display:flex;"><span>        label <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>labels[index]
</span></span><span style="display:flex;"><span>        domain_output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>domain_outputs[index]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> image, label, domain_output
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __len__(self):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> len(self<span style="color:#f92672">.</span>images)
</span></span></code></pre></div><p>Run the prediction of Domain Classifier on Source Domain and save result in the <code>CombinedDataset</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Calculate overall accuracy</span>
</span></span><span style="display:flex;"><span>total_correct <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>total_samples <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>domain_classifier<span style="color:#f92672">.</span>eval()  <span style="color:#75715e"># Set the model to evaluation mode</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>domain_outputs <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>source_images_list <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>source_labels_list <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> images, source_labels <span style="color:#f92672">in</span> source_dataloader:
</span></span><span style="display:flex;"><span>        input_images <span style="color:#f92672">=</span> images<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>        labels <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ones(images<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>long)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        domain_output <span style="color:#f92672">=</span> domain_classifier(input_images)
</span></span><span style="display:flex;"><span>        _, domain_pred <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>max(domain_output, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        total_correct <span style="color:#f92672">+=</span> (domain_pred <span style="color:#f92672">==</span> labels)<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>        total_samples <span style="color:#f92672">+=</span> len(labels)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># print(100.0 * correct/total)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        domain_outputs<span style="color:#f92672">.</span>append(domain_output)
</span></span><span style="display:flex;"><span>        source_images_list<span style="color:#f92672">.</span>append(input_images)
</span></span><span style="display:flex;"><span>        source_labels_list<span style="color:#f92672">.</span>append(source_labels)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Concatenate the domain outputs, images, and labels</span>
</span></span><span style="display:flex;"><span>combined_domain_output <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat(domain_outputs, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>combined_source_images <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat(source_images_list, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>combined_source_labels <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat(source_labels_list, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>combined_dataset <span style="color:#f92672">=</span> CombinedDataset(combined_source_images, combined_source_labels, combined_domain_output)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accuracy <span style="color:#f92672">=</span> <span style="color:#ae81ff">100.0</span> <span style="color:#f92672">*</span> total_correct <span style="color:#f92672">/</span> total_samples
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Accuracy of domain classifier: </span><span style="color:#e6db74">{</span>accuracy<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">%&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>combined_dataloader <span style="color:#f92672">=</span> DataLoader(
</span></span><span style="display:flex;"><span>    combined_dataset,
</span></span><span style="display:flex;"><span>    batch_size<span style="color:#f92672">=</span>batch_size,
</span></span><span style="display:flex;"><span>    shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><ul>
<li>Train the Label Classifier, the prediction was run on TARGET domain and  displayed the accuracy after each epoch; We perform 5 runs and record the average accuracy</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>thes <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>max_value <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span> 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-3</span>
</span></span><span style="display:flex;"><span>batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>
</span></span><span style="display:flex;"><span>image_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">28</span>
</span></span><span style="display:flex;"><span>n_epoch <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>output_accuracies <span style="color:#f92672">=</span> [] 
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> run_num <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">5</span>) :
</span></span><span style="display:flex;"><span>    LOSS_LABEL <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    ACC_LABEL <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Model, Loss, and Optimizer</span>
</span></span><span style="display:flex;"><span>    label_classifier <span style="color:#f92672">=</span> LabelClassifier()<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>    loss_label <span style="color:#f92672">=</span> ReweightedCrossEntropyLoss()
</span></span><span style="display:flex;"><span>    optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>Adam(label_classifier<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span>lr)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Training loop</span>
</span></span><span style="display:flex;"><span>    input_dataloader <span style="color:#f92672">=</span> combined_dataloader
</span></span><span style="display:flex;"><span>    acc_result <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(n_epoch):
</span></span><span style="display:flex;"><span>        correct <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        total_samples <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        data_iter <span style="color:#f92672">=</span> iter(input_dataloader)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(input_dataloader)):
</span></span><span style="display:flex;"><span>            len_dataloader <span style="color:#f92672">=</span> len(input_dataloader)
</span></span><span style="display:flex;"><span>            data_source <span style="color:#f92672">=</span> next(data_iter)
</span></span><span style="display:flex;"><span>            img, label, domain_clf_output <span style="color:#f92672">=</span> data_source
</span></span><span style="display:flex;"><span>            batch_size <span style="color:#f92672">=</span> len(img)
</span></span><span style="display:flex;"><span>            input_img <span style="color:#f92672">=</span> img<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>            label <span style="color:#f92672">=</span> label<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>            label_output <span style="color:#f92672">=</span> label_classifier(input_img)
</span></span><span style="display:flex;"><span>            label_pred <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(label_output, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            correct <span style="color:#f92672">+=</span> (label_pred <span style="color:#f92672">==</span> label)<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>            total_samples <span style="color:#f92672">+=</span> batch_size
</span></span><span style="display:flex;"><span>            label_acc <span style="color:#f92672">=</span> <span style="color:#ae81ff">100.0</span> <span style="color:#f92672">*</span> correct <span style="color:#f92672">/</span> total_samples
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>                detached_domain_output <span style="color:#f92672">=</span> domain_clf_output<span style="color:#f92672">.</span>detach()
</span></span><span style="display:flex;"><span>            loss <span style="color:#f92672">=</span> loss_label(label_output, label, domain_clf_output, thes, max_value)
</span></span><span style="display:flex;"><span>            ACC_LABEL<span style="color:#f92672">.</span>append(label_acc)
</span></span><span style="display:flex;"><span>            LOSS_LABEL<span style="color:#f92672">.</span>append(loss)
</span></span><span style="display:flex;"><span>            optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>            loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>            optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> (i <span style="color:#f92672">%</span> <span style="color:#ae81ff">200</span>) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">and</span> i <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>                print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;epoch: </span><span style="color:#e6db74">{</span>epoch<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">, [iter: </span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">:</span><span style="color:#e6db74">03d</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> / all </span><span style="color:#e6db74">{</span>len_dataloader<span style="color:#e6db74">}</span><span style="color:#e6db74">], &#39;</span>
</span></span><span style="display:flex;"><span>                    <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;loss label: </span><span style="color:#e6db74">{</span>loss<span style="color:#f92672">.</span>item()<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">, &#39;</span>
</span></span><span style="display:flex;"><span>                    <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;| label acc (SOURCE): </span><span style="color:#e6db74">{</span>label_acc<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># run on test set after each epochs to monitor result on test set (target set)</span>
</span></span><span style="display:flex;"><span>        domain_outputs <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        source_train_images_list <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        source_train_labels_list <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        total_correct <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        total_samples <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        label_classifier<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> batch_data <span style="color:#f92672">in</span> target_dataloader:
</span></span><span style="display:flex;"><span>            train_images, train_labels <span style="color:#f92672">=</span> batch_data
</span></span><span style="display:flex;"><span>            batch_size <span style="color:#f92672">=</span> len(train_labels)
</span></span><span style="display:flex;"><span>            input_images <span style="color:#f92672">=</span> train_images<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>            class_label <span style="color:#f92672">=</span> train_labels<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>                pred_output <span style="color:#f92672">=</span> label_classifier(input_images)
</span></span><span style="display:flex;"><span>                class_pred <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(pred_output, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>                correct <span style="color:#f92672">=</span> (class_pred <span style="color:#f92672">==</span> class_label)<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>                total <span style="color:#f92672">=</span> len(class_label)
</span></span><span style="display:flex;"><span>                total_correct <span style="color:#f92672">+=</span> correct
</span></span><span style="display:flex;"><span>                total_samples <span style="color:#f92672">+=</span> total
</span></span><span style="display:flex;"><span>        accuracy <span style="color:#f92672">=</span> <span style="color:#ae81ff">100.0</span> <span style="color:#f92672">*</span> total_correct <span style="color:#f92672">/</span> total_samples
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Accuracy of label classifier (on TARGET) with reweight loss after epoch </span><span style="color:#e6db74">{</span>epoch<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">: </span><span style="color:#e6db74">{</span>accuracy<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">%&#34;</span>)
</span></span><span style="display:flex;"><span>        acc_result<span style="color:#f92672">.</span>append(accuracy)
</span></span><span style="display:flex;"><span>    output_accuracies<span style="color:#f92672">.</span>append(accuracy)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Run num: &#34;</span>, run_num, <span style="color:#e6db74">&#39;, &#39;</span><span style="color:#f92672">.</span>join([<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span> <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> acc_result]))
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;------------------------------------------&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;-&gt; Accuracies of 5 runs: &#34;</span>, <span style="color:#e6db74">&#39;, &#39;</span><span style="color:#f92672">.</span>join([<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span> <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> output_accuracies])) 
</span></span><span style="display:flex;"><span>_acc  <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(output_accuracies)
</span></span><span style="display:flex;"><span>_avg <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(_acc, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;-&gt; Average accuracy of 5 runs: </span><span style="color:#e6db74">{</span>_avg<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>) 
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">------------------------------------------</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-&gt;</span> Accuracies of <span style="color:#ae81ff">5</span> runs:  <span style="color:#ae81ff">96.87</span>, <span style="color:#ae81ff">97.09</span>, <span style="color:#ae81ff">96.80</span>, <span style="color:#ae81ff">97.31</span>, <span style="color:#ae81ff">96.86</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-&gt;</span> Average accuracy of <span style="color:#ae81ff">5</span> runs: <span style="color:#ae81ff">96.99</span>
</span></span></code></pre></div><h2 id="comparision">Comparision<a hidden class="anchor" aria-hidden="true" href="#comparision">#</a></h2>
<p>Now, let compare with Traditional Implementation (without the Domain Classifer) by using the standard <code>CrossEntropyLoss</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.optim <span style="color:#66d9ef">as</span> optim
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Hyperparameters</span>
</span></span><span style="display:flex;"><span>lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-3</span>
</span></span><span style="display:flex;"><span>batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>
</span></span><span style="display:flex;"><span>image_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">28</span>
</span></span><span style="display:flex;"><span>n_epoch <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>output_accuracies <span style="color:#f92672">=</span> []  
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> run_num <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">5</span>):
</span></span><span style="display:flex;"><span>    output <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Model, Loss, and Optimizer</span>
</span></span><span style="display:flex;"><span>    traditional_label_classifier <span style="color:#f92672">=</span> LabelClassifier()<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>    loss_label_no_dc <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>CrossEntropyLoss()
</span></span><span style="display:flex;"><span>    optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>Adam(traditional_label_classifier<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span>lr)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Training loop</span>
</span></span><span style="display:flex;"><span>    CLASSICAL_LOSS_LABEL <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    CLASSICAL_ACC_LABEL <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(n_epoch):
</span></span><span style="display:flex;"><span>        correct <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        total_samples <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        input_dataloader <span style="color:#f92672">=</span> combined_dataloader
</span></span><span style="display:flex;"><span>        data_iter <span style="color:#f92672">=</span> iter(input_dataloader)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(input_dataloader)):
</span></span><span style="display:flex;"><span>            len_dataloader <span style="color:#f92672">=</span> len(input_dataloader)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            data_source <span style="color:#f92672">=</span> next(data_iter)
</span></span><span style="display:flex;"><span>            img, label, _<span style="color:#f92672">=</span> data_source
</span></span><span style="display:flex;"><span>            batch_size <span style="color:#f92672">=</span> len(img)
</span></span><span style="display:flex;"><span>            input_img <span style="color:#f92672">=</span> img<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>            label <span style="color:#f92672">=</span> label<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            label_output <span style="color:#f92672">=</span> traditional_label_classifier(input_img)
</span></span><span style="display:flex;"><span>            label_pred <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(label_output, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            correct <span style="color:#f92672">+=</span> (label_pred <span style="color:#f92672">==</span> label)<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>            total_samples <span style="color:#f92672">+=</span> len(label)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            label_acc <span style="color:#f92672">=</span> <span style="color:#ae81ff">100.0</span> <span style="color:#f92672">*</span> correct <span style="color:#f92672">/</span> total_samples
</span></span><span style="display:flex;"><span>            loss <span style="color:#f92672">=</span> loss_label_no_dc(label_output, label)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            CLASSICAL_ACC_LABEL<span style="color:#f92672">.</span>append(label_acc)
</span></span><span style="display:flex;"><span>            CLASSICAL_LOSS_LABEL<span style="color:#f92672">.</span>append(loss<span style="color:#f92672">.</span>item())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>            loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>            optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> (i <span style="color:#f92672">%</span> <span style="color:#ae81ff">200</span>) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">and</span> i <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>                print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;epoch: </span><span style="color:#e6db74">{</span>epoch<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">, [iter: </span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">:</span><span style="color:#e6db74">03d</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> / all </span><span style="color:#e6db74">{</span>len_dataloader<span style="color:#e6db74">}</span><span style="color:#e6db74">], &#39;</span>
</span></span><span style="display:flex;"><span>                      <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;loss label: </span><span style="color:#e6db74">{</span>loss<span style="color:#f92672">.</span>item()<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">, &#39;</span>
</span></span><span style="display:flex;"><span>                      <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;| label acc (SOURCE): </span><span style="color:#e6db74">{</span>label_acc<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Evaluation on target dataset after each epoch</span>
</span></span><span style="display:flex;"><span>        domain_outputs <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        source_train_images_list <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        source_train_labels_list <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        total_correct <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        total_samples <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        traditional_label_classifier<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> batch_data <span style="color:#f92672">in</span> target_dataloader:
</span></span><span style="display:flex;"><span>            train_images, train_labels <span style="color:#f92672">=</span> batch_data
</span></span><span style="display:flex;"><span>            batch_size <span style="color:#f92672">=</span> len(train_labels)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            input_images <span style="color:#f92672">=</span> train_images<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>            class_label <span style="color:#f92672">=</span> train_labels<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>                pred_output <span style="color:#f92672">=</span> traditional_label_classifier(input_images)
</span></span><span style="display:flex;"><span>                class_pred <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(pred_output, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>                correct <span style="color:#f92672">=</span> (class_pred <span style="color:#f92672">==</span> class_label)<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>                total <span style="color:#f92672">=</span> len(class_label)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>                total_correct <span style="color:#f92672">+=</span> correct
</span></span><span style="display:flex;"><span>                total_samples <span style="color:#f92672">+=</span> total
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        accuracy <span style="color:#f92672">=</span> <span style="color:#ae81ff">100.0</span> <span style="color:#f92672">*</span> total_correct <span style="color:#f92672">/</span> total_samples
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;- Accuracy of label classifier (predict on TARGET) after epoch </span><span style="color:#e6db74">{</span>epoch<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">: </span><span style="color:#e6db74">{</span>accuracy<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">%&#34;</span>) 
</span></span><span style="display:flex;"><span>        output<span style="color:#f92672">.</span>append(accuracy)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    output_accuracies<span style="color:#f92672">.</span>append(accuracy)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#39;Run number&#39;</span>, run_num, <span style="color:#e6db74">&#39;|&#39;</span>, <span style="color:#e6db74">&#39;, &#39;</span><span style="color:#f92672">.</span>join([<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span> <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> output]))
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>) 
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;-&gt; Accuracies of 5 runs: &#34;</span>, <span style="color:#e6db74">&#39;, &#39;</span><span style="color:#f92672">.</span>join([<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span> <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> output_accuracies])) 
</span></span><span style="display:flex;"><span>_acc  <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(output_accuracies)
</span></span><span style="display:flex;"><span>_avg <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(_acc, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;-&gt; Average accuracy of 5 runs: </span><span style="color:#e6db74">{</span>_avg<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>) 
</span></span></code></pre></div><p>Result:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-&gt;</span> Accuracies of <span style="color:#ae81ff">5</span> runs:  <span style="color:#ae81ff">96.83</span>, <span style="color:#ae81ff">96.09</span>, <span style="color:#ae81ff">96.69</span>, <span style="color:#ae81ff">96.94</span>, <span style="color:#ae81ff">96.79</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-&gt;</span> Average accuracy of <span style="color:#ae81ff">5</span> runs: <span style="color:#ae81ff">96.67</span> 
</span></span></code></pre></div><h2 id="result-on-2-setting">Result on 2 setting<a hidden class="anchor" aria-hidden="true" href="#result-on-2-setting">#</a></h2>
<p>MNIST-M -&gt; MNIST: MNIST-M as source and</p>
<table>
<thead>
<tr>
<th>Accuracy</th>
<th>MNIST-M -&gt; MNIST</th>
<th>MNIST -&gt; mnist-M</th>
</tr>
</thead>
<tbody>
<tr>
<td>Traditional  (average of 5 runs)</td>
<td>96.67%</td>
<td>55.31%</td>
</tr>
<tr>
<td>Covariate - Shift (average of 5 runs)</td>
<td>96.99%</td>
<td>57.22%</td>
</tr>
</tbody>
</table>
<h2 id="discussion-and-future-parts">Discussion and future parts<a hidden class="anchor" aria-hidden="true" href="#discussion-and-future-parts">#</a></h2>
<p><strong>Analysis</strong>: Before we continue, let&rsquo;s address a question:
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<span id="latex">\[ \color{red}{\text{Why does the result of Covariate Shift} \\ \text{perform well in the case of } \text{MNIST-M} \rightarrow \text{MNIST} \\ \text{but poorly in the case of MNIST} \rightarrow {MNIST-M?}} \] </span>
<script>
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, "latex"]);
</script> 

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true
        },
        TeX: {
            equationNumbers: { autoNumber: "AMS" },
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            Macros: {}
        },
        "HTML-CSS": { scale: 90 },
        SVG: { scale: 90 }
    });
</script>
 
</p>
<p><strong>Comment:</strong>
Let&rsquo;s revisit the <strong>assumption</strong> of the covariate shift problem setting that is that <strong>the support (range of values) of the source domain is larger than or equal to the support of the target domain</strong>. We observe that in the <strong>MNIST-M (source) -&gt;  MNIST (target)</strong> case, the model still captures features related to the target set because there are many samples that are similar to the test set (colorized images can generalize to black and white). However, in the MNIST (source) -&gt; MNIST-M (target) case, the source dataset does not cover the domain of the domain set (black and white cannot generalize to colorized images).</p>
<p>In the next blogs, we will explore <strong>two other powerful techniques</strong> to handle situations where the support of the test set is not within the training set, Domain Adversarial Training and CycleGAN.</p>
<p>According to the table below, we can see that Domain Adversarial Training handles the situation very well in the MNIST -&gt; MNIST-M case. In the covariate shift problem, we focus on mapping the sample space, but with domain adversarial training, we concentrate on the feature space. We aim to find a feature space that is general enough to represent both domains (invariant domain features).</p>
<table>
<thead>
<tr>
<th>Accuracy</th>
<th>MNIST-M -&gt; MNIST</th>
<th>MNIST -&gt; mnist-M</th>
</tr>
</thead>
<tbody>
<tr>
<td>Traditional  (average of 5 runs)</td>
<td>96.67%</td>
<td>55.31%</td>
</tr>
<tr>
<td>Covariate - Shift (average of 5 runs)</td>
<td>96.99%</td>
<td>57.22%</td>
</tr>
<tr>
<td>Domain Adversarial Training</td>
<td>96.61%</td>
<td>72.08%</td>
</tr>
</tbody>
</table>
<p>In Part 3, we will delve into a more advanced technique related to GANs, known as CycleGAN, where we generate samples from domain A while incorporating domain information from another domain, B.</p>
<p><em>Isn&rsquo;t it interesting? Please Let me know your comment and feedback. See you in the next posts!</em></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="">Mai-Anh&#39;s blog about Machine Learning</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
